[{"content":"This project aims to recreate a Minecraft-like terrain generation without using an engine. Made using OpenGL and the GLFW, GLM and GLEW libraries. Source code.\nMain Classes These are the main classes used in the project:\nTerrain The terrain is composed of chunks, which are generated around the player according to his position in space.\nChunk A single chunk is a collection of Blocks, every chunk allocates in the memory its own blocks, giving them a local position and a world position based on its own. From every chunk is created a Mesh, which is the actual collection of vertices that will be sent to the shaders to be drawn on the screen.\nBlock A Block is the smallest component of the terrain. Each block has a different ID according to its local y coordinate in the chunk. The ID determines which texture will be applied to the block\u0026rsquo;s faces and if the block is to be considered solid or not when the mesh of its chunk is being created. Block IDs are air, dirt, grass, cobblestone.\nMesh The mesh is the collection of vertices of a chunk that will eventually be drawn to the screen. It is created by evaluating if the blocks\u0026rsquo; faces would be visible by the player, if not there\u0026rsquo;s no need to draw them on the screen. The mesh vertices will then be used as the buffer in the function glBufferData or glBufferSubData.\nShader and Buffer These are the classes that wrap around the opengl functions. They are responsible of creating a shader program, compiling it and deleting it, allocating the gl buffers and update them with data from the meshes.\nTerrain generation In the Terrain class constructor every Chunk class instance is allocated and stored row-by-row along the z axis in a bidimensional array. The chunks grid consists of a number of CHUNK_RADIUS chunks created along the positive and negative z and x axis from the player Then for every chunk the function GenerateMeshes is called: the function checks if the current chunk being processed is in front of the camera to determine if the mesh should be generated or not After the call of GenerateMeshes the value of m_CurrentChunk is incremented to process, in the next frame, the next chunk in the grid When the player crosses the edge of the chunk it\u0026rsquo;s in, the terrain around it is regenerated: chunks that are distant more then CHUNK_RADIUS from its position in the grid are deleted and new ones are generated in the direction it\u0026rsquo;s moving When this occurs, m_CurrentChunk is reset to 0 because all the meshes should be generated again according to the new chunks\u0026rsquo; disposition that affects some of the chunks\u0026rsquo; surroundings Mesh generation every chunk has a m_LowestSolidHeight field that holds the lowest y coordinate value of the block whose ID is not air. The m_Blocks array is iterated through starting from the lowest solid block\u0026rsquo;s index the condition checked first is if the block\u0026rsquo;s world position lies inside the camera\u0026rsquo;s frustum. for every face of the blocks is checked if there is a neighbouring block which is solid. If it is not, then the vertices of the face should be added to the mesh because they will be visible if a face is at the edge of a chunk, then the program checks if the neighbouring chunk exists and if it has a solid block adjacent to the face By doing this only the vertices that make up the external profile of the chunk will be rendered three vectors store the data that\u0026rsquo;s needed to correctly render a block face: m_Faces,m_TexCoords and m_ModelMats m_Faces: each face of a block has its array of data in the header file Renderer.h. For every face added to the mesh one of the values of the enum variable \u0026ldquo;sides\u0026rdquo; is pushed back to the m_Faces vector. So when the process of mesh generation is done m_Faces is a collection of indexes that determine which of the six arrays that represent a face will be used in the shader.\nm_TexCoords: for every vertex added to the mesh the corresponding 2D textures coordinates are pushed back to the m_TexCoords vector. The texture coordinates referer to the texture atlas image and depend on the block\u0026rsquo;s ID\nm_ModelMats: for every face added to the mesh, a mat4 4x4 matrix is pushed back to the m_ModelMats vector. These are the matrices that will compute the correct 3D position of a vertex. When a face is deemed to be part of the chunk\u0026rsquo;s mesh a model matrix is calculated from its block\u0026rsquo;s world position and in the vertex shader every position of the vertices of that face will be trasnformed by the matrix\nTextures To every visible face of a block is applied a texture according to its ID. All the different textures are stored in a single texture atlas, handled by the TextureAtlas class, and are retrieved through the correct offset based on the block\u0026rsquo;s ID.\nBlock Breaking A ray is sent from the player\u0026rsquo;s camera into the world. If it intersects a block whose ID isn\u0026rsquo;t air the block\u0026rsquo;s edges are highlighted. On mouse input, the block\u0026rsquo;s ID is set to air so that when the mesh is regenerated its faces won\u0026rsquo;t be rendered, conveying the effect of breaking the block. The highlighting of the edges is done by giving every vertex of a face barycentric coordinates and then checking them in the fragment shader to determine if the vertex is at the edge of a face.\n","date":"0001-01-01T00:00:00Z","image":"https://cmanziel.github.io/p/minecraft-like-terrain/terrain_noise_hu10173853994954059566.png","permalink":"https://cmanziel.github.io/p/minecraft-like-terrain/","title":"Minecraft-like terrain"},{"content":"This project combines two other ones i made before. One is about drawing on a window using the mouse cursor as a brush, the other is the pnglib library to convert into an image what\u0026rsquo;s been drawn on the window. Source code.\nWindow The window is conceptually divided into a grid of pixels, whose data is stored in a bi-dimensional array of point structures. The image pixels are mapped 1:1 to the window pixels. The window creation is done through GLFW.\nImage Decompression To edit the image it has to be firstly displayed onto the window. To do so the image has to be decompressed to get the raw pixel data containing the rgb values for each pixel. The decompression is handled by the pnglib library: its decompress function receives the image file and returns its pixel data. If no image is provided or the file is corrupted a default white color is applied to every pixel\nBrush A brush is constructed around the mouse cursor as a circular grid. The center of the grid is the pixel the cursor is currently pointing. Its dimensions can vary on mouse input or based on the cursor\u0026rsquo;s speed as selected by the user. Handled by the Brush class.\nPointBuffer This is the class that actually handles the window\u0026rsquo;s grid allocation in memory and modifies its values. The functions InsertPoint and RemovePoint use the brush\u0026rsquo;s position and dimension to draw the correct points on the image.\nImage Generation The initial image\u0026rsquo;s pixel data is overwritten by what\u0026rsquo;s drawn onto it. The window\u0026rsquo;s bi-dimensional grid array serves as the raw pixel data input for the compress function of the pnglib library. A PNG file is written from scratch by the library inserting the compressed pixel data into it.\n","date":"0001-01-01T00:00:00Z","image":"https://cmanziel.github.io/p/imageeditor/collage_hu6667875158802057849.png","permalink":"https://cmanziel.github.io/p/imageeditor/","title":"ImageEditor"},{"content":"This project is a refactor of the ImageEditor project. The first version of the project was mainly relying on the CPU for all the drawing and effects processes. This one uses compute shaders for every step of the way. Source code.\nDrawing Each drawing style the brush can have has its own shader. The shader samples the canvas texture to retrieve the background color through GLSL\u0026rsquo;s imageLoad function, and stores the color being drawn into the render target texture. After the off-screen rendering is done, the render texture is mapped to the vertices of a quad which is rendered through the usual vertex and fragment shader.\nFor every compute shader, a brush radius * brush radius number of work groups is dispatched. The local_size of every work group is 1 x 1 x 1, so the image can be accessed using the gl_GlobalInvocationID.xy built-in variable as the UV coordinates.\n1 2 3 4 void Execute() { Dispatch(m_BrushRadius, m_BrushRadius, 1); } 1 2 3 4 5 void Dispatch(unsigned int xGroups, unsigned int yGroups, unsigned int zGroups) { glDispatchCompute(xGroups, yGroups, zGroups); glMemoryBarrier(GL_SHADER_IMAGE_ACCESS_BARRIER_BIT | GL_SHADER_STORAGE_BARRIER_BIT); } Brush Effects Edge Detection (Sobel Operator) Two kernels are defined to process every pixel in area covered by the brush based on its surrounding pixels. The effect is applied on everything that has been drawn onto the canvas and not only on the original canvas.\nTo prevent imageStore operations to affect imageLoad operations in the same shader dispatch a separate canvas texture image is used. This prevents an imageLoad operation to get the value of another pixel in the grid which was just modified by the same operation that is being done on the current one.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 float kernel_mult(ivec2 pixel_uv, int ch, mat3 kernel) { float kernel_sum = 0; ivec2 dims = imageSize(edited); for(int y = -1; y \u0026lt; 2; y++) { for(int x = -1; x \u0026lt; 2; x++) { ivec2 grid_uv = pixel_uv + ivec2(x, y); vec4 current_color = imageLoad(sobelCanvas, ivec2(grid_uv.x, grid_uv.y)); kernel_sum += int(current_color[ch] * kernel[x + 1][1 - y]); } } return kernel_sum; } vec4 edge_pixel(ivec2 uv) { // imageLoad with an out of boundary coordinate returns an all zeroes vec4 // so for the edge pixels of the imagedo the kernel multiplication with out of bounds values as for the other pixels vec4 result; float sum = 0; // for each of the rgb channels compute the kernel calculation for(int i = 0; i \u0026lt; 3; i++) { float Gx = kernel_mult(uv, i, sobel_kernel_Gx); float Gy = kernel_mult(uv, i, sobel_kernel_Gy); // sqrt is a floating point operation result[i] = sqrt(Gx * Gx + Gy * Gy / SOBEL_CLAMP_FLOAT_VALUE); sum += result[i]; } result.xyz = vec3(sum / 3); return result; } Blur (Box Blur) A Box Blur algorithm is applied to every pixel inside the brush area. The result is the average value between a 3x3 grid around each pixel. The effect is applied on everything that has been drawn onto the canvas and not only on the original canvas.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 vec4 blur_pixel(ivec2 uv, ivec2 dims) { vec4 result; // if the pixel is on the edge of the image, return it unmodified if(uv.x \u0026lt; 1 || uv.y \u0026lt; 1 || uv.x + 1 == dims.x || uv.y + 1 == dims.y) return imageLoad(blurCanvas, uv); for(int y = -1; y \u0026lt; 2; y++) { for(int x = -1; x \u0026lt; 2; x++) { if(x == 0 \u0026amp;\u0026amp; y == 0) continue; ivec2 grid_uv = uv + ivec2(x, y); vec4 current_color = imageLoad(blurCanvas, ivec2(grid_uv.x, grid_uv.y)); result += current_color; } } result.xyz /= 9; return result; } ","date":"0001-01-01T00:00:00Z","image":"https://cmanziel.github.io/p/csimage/effects_hu8949791237012257972.png","permalink":"https://cmanziel.github.io/p/csimage/","title":"CSImage"},{"content":"Static library to encode or decode PNG files by compressing data into zlib datastreams or decompressing from them. The aim of this project was to generate an uncorrupted PNG file from scratch. Source code.\nPNG file structure According to the PNG specification a PNG file has to contain at least each one of these chunks of data:\nIHDR This is the image header chunk. It contains general information about the image like width, height, pixel data length and format and an 8-byte signature that every PNG file contains.\nIDAT This is the chunk that contains the actual image pixel data. The original raw data, stored in the format indicated in the IHDR chunk, has to be compressed through the zlib library\u0026rsquo;s DEFLATE routine and put into one or multiple IDAT chunks one after each other. An incorrect zlib\u0026rsquo;s datastream will result in a corrupted PNG file.\nIEND Chunk at the end of the image file.\nImage Encoding Each chunk consists of three or four fields:\nLength: A four-byte unsigned integer giving the number of bytes in the chunk\u0026rsquo;s data field. Chunk type: The data bytes appropriate to the chunk type, if any. This field can be of zero length. Chunk data: the actual data for the chunk in use CRC: A four-byte CRC calculated on the preceding bytes in the chunk, including the chunk type field and chunk data fields, but not including the length field. When encoding a PNG file the library function compress calls the zlib\u0026rsquo;s compression routines, sets the necessary fields for every chunk (like signatures and CRCs) and writes to the image file opened.\nImage Decoding When decoding a PNG file the library searches all the IDAT chunks in the file, concatenates their data to get the whole zlib datastream, decompresses it to the original pixel data and returns it.\n","date":"0001-01-01T00:00:00Z","image":"https://cmanziel.github.io/p/pnglib/cover_hu2670802942028375950.jpg","permalink":"https://cmanziel.github.io/p/pnglib/","title":"pnglib"}]